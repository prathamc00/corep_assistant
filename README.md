# LLM-Assisted PRA COREP Reporting Assistant (Prototype)

This project demonstrates an **AI-assisted regulatory reporting system** for UK banks regulated by the **Prudential Regulation Authority (PRA)** under the **COREP** reporting framework.

The prototype focuses on **COREP Template C01.00 â€” Own Funds** and shows how an LLM can:

* Retrieve relevant PRA Rulebook text (RAG)
* Interpret rules for a reporting scenario
* Generate structured COREP field outputs
* Render a human-readable COREP extract
* Apply validation checks to detect prudential inconsistencies

The system runs **fully offline** using Ollama + Llama 3.1 (no API keys required).

---

## ğŸ§  What This Prototype Demonstrates

End-to-end flow:

```
User scenario
   â†“
Retrieve PRA rule text (vector RAG)
   â†“
LLM reasoning (local model)
   â†“
Structured COREP JSON output
   â†“
Template rendering
   â†“
Validation engine flags inconsistencies
```

This is not a chatbot â€” it is a **rule-aware regulatory reporting assistant**.

---

## ğŸ—ï¸ Architecture

See: `Architecture.png`

## ğŸ”„ Workflow

See: `workflow.png`

---

## âš™ï¸ Setup Instructions

### 1. Clone the repository

```
git clone https://github.com/prathamc00/corep_assistant.git
cd corep_assistant
```

### 2. Create and activate virtual environment

**Windows**

```
python -m venv .venv
.venv\Scripts\activate
```

**Mac/Linux**

```
python3 -m venv .venv
source .venv/bin/activate
```

### 3. Install dependencies

```
pip install -r requirements.txt
```

### 4. Install Ollama and pull model

Download Ollama: [https://ollama.com](https://ollama.com)

Then run:

```
ollama pull llama3.1
```

---

## ğŸ“„ Download Required PRA Documents

This project uses PRA Rulebook PDFs as its knowledge base.

These are **not included** in the repo.

Open `docs/README.txt` and download the required PDFs into the `/docs` folder.

---

## ğŸ§± Build the Vector Store (RAG memory)

After placing the PDFs in `/docs`, run:

```
python ingestion/embedder.py
```

This creates the local `vectorstore/` used for rule retrieval.

---

## â–¶ï¸ Run the System

### Test retrieval only (RAG test)

```
python test_retrieval.py
```

### Run full COREP reporting assistant

```
python test_reporter.py
```

---

## âœ… Example Output

```
COREP C01.00 â€” Own Funds

Code      Field                              Value
------------------------------------------------------------
010       Common Equity Tier 1               65

VALIDATION FLAGS:
- CET1 calculation mismatch. Expected 55 based on deductions, got 65.
```

This demonstrates:

* LLM populates COREP field
* Validation engine detects prudential error

---

## ğŸ“ Project Structure

```
docs/                â†’ Instructions to download PRA PDFs
ingestion/           â†’ PDF chunking & embedding
llm/                 â†’ LLM reporting logic & schema
renderer/            â†’ COREP template view
validator/           â†’ Prudential validation rules

test_retrieval.py    â†’ RAG test
test_reporter.py     â†’ End-to-end demo
```

---

## ğŸ§© Technologies Used

* Python
* LangChain (modular v1 packages)
* ChromaDB (vector store)
* Sentence Transformers (embeddings)
* Ollama
* Llama 3.1 (local LLM)

---

## ğŸ¯ Key Features

* Retrieval-Augmented Generation over PRA Rulebook
* Schema-constrained COREP output
* Human-readable template rendering
* Validation engine for prudential consistency
* Fully offline execution (no API usage)

---

## ğŸ“Œ Notes

* `vectorstore/` is not committed (generated locally)
* PRA PDFs are not committed (see `docs/README.txt`)
* No API keys required

---

## ğŸ“š Purpose

This prototype illustrates how LLMs can be safely used in regulated environments with:

* Traceability
* Structured outputs
* Validation safeguards
* Human oversight
